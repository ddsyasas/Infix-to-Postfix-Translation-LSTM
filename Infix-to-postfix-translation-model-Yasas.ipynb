{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Constants --------------------\n",
    "OPERATORS = ['+', '-', '*', '/']\n",
    "IDENTIFIERS = list('abcde')\n",
    "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
    "SYMBOLS = ['(', ')', '+', '-', '*', '/']\n",
    "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK']\n",
    "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
    "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "PAD_ID = token_to_id['PAD']\n",
    "EOS_ID = token_to_id['EOS']\n",
    "SOS_ID = token_to_id['SOS']\n",
    "MAX_DEPTH = 3\n",
    "MAX_LEN = 4*2**MAX_DEPTH - 2\n",
    "\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Vocabulary: {VOCAB}\")\n",
    "print(f\"PAD_ID: {PAD_ID}, SOS_ID: {SOS_ID}, EOS_ID: {EOS_ID}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Expression Generation --------------------\n",
    "def generate_infix_expression(max_depth):\n",
    "    if max_depth == 0:\n",
    "        return random.choice(IDENTIFIERS)\n",
    "    elif random.random() < 0.5:\n",
    "        return generate_infix_expression(max_depth - 1)\n",
    "    else:\n",
    "        left = generate_infix_expression(max_depth - 1)\n",
    "        right = generate_infix_expression(max_depth - 1)\n",
    "        op = random.choice(OPERATORS)\n",
    "        return f'({left} {op} {right})'\n",
    "\n",
    "def tokenize(expr):\n",
    "    return [c for c in expr if c in token_to_id]\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
    "    output, stack = [], []\n",
    "    for token in tokens:\n",
    "        if token in IDENTIFIERS:\n",
    "            output.append(token)\n",
    "        elif token in OPERATORS:\n",
    "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        elif token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def encode(tokens, max_len=MAX_LEN):\n",
    "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
    "    return ids + [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "def decode_sequence(token_ids, id_to_token, pad_token='PAD', eos_token='EOS'):\n",
    "    \"\"\"\n",
    "    Converts a list of token IDs into a readable string by decoding tokens.\n",
    "    Stops at the first EOS token if present, and ignores PAD tokens.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        token = id_to_token.get(token_id, '?')\n",
    "        if token == eos_token:\n",
    "            break\n",
    "        if token != pad_token:\n",
    "            tokens.append(token)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_dataset(n, max_depth=MAX_DEPTH):\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        expr = generate_infix_expression(max_depth)\n",
    "        #expr = expr_gen.generate(max_depth=max_dthep)\n",
    "        infix = tokenize(expr)\n",
    "        postfix = infix_to_postfix(infix)\n",
    "        X.append(encode(infix))\n",
    "        Y.append(encode(postfix))\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "#you might use the shift function for teacher-forcing\n",
    "def shift_right(seqs):\n",
    "    shifted = np.zeros_like(seqs)\n",
    "    shifted[:, 1:] = seqs[:, :-1]\n",
    "    shifted[:, 0] = SOS_ID\n",
    "    return shifted"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Generate and Inspect Dataset --------------------\n",
    "print(\"\\n Generating Training Dataset\")\n",
    "\n",
    "X_train, Y_train = generate_dataset(100000)\n",
    "decoder_input_train = shift_right(Y_train)\n",
    "\n",
    "X_val, Y_val = generate_dataset(10000)\n",
    "decoder_input_val = shift_right(Y_val)\n",
    "\n",
    "X_test, Y_test = generate_dataset(1000)\n",
    "decoder_input_test = shift_right(Y_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Show 5 random examples\n",
    "print(\"\\n5 Dataset Examples\")\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(X_train))\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Infix:   {decode_sequence(X_train[idx], id_to_token)}\")\n",
    "    print(f\"  Postfix: {decode_sequence(Y_train[idx], id_to_token)}\")\n",
    "    print(f\"  Teacher: {decode_sequence(decoder_input_train[idx], id_to_token)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final Model Architecture: Encoder-Decoder with LSTM\n",
    "\n",
    "This section defines the neural network model that performs the translation from infix to postfix notation.\n",
    "\n",
    "In this task I have use an **encoder-decoder architecture with LSTM layers**:\n",
    "\n",
    "- The **encoder** reads the tokenized infix expression and encodes it into a fixed-size state vector.\n",
    "- The **decoder** uses this encoded state along with teacher forcing to generate the corresponding postfix sequence step-by-step.\n",
    "\n",
    "Main Components are these:\n",
    "\n",
    "- **Embedding Layers**: Transform token IDs into dense vector representations for both encoder and decoder.\n",
    "- **LSTM Layers**: Handle sequential input and preserve contextual information.\n",
    "- **Dense Layers**: Map the decoder output to token probabilities using a softmax layer.\n",
    "- **Parameter Limit**: The total parameter count is kept below the required 2 million.\n",
    "\n",
    "The model is compiled using the **Adam** optimizer and trained with the **sparse_categorical_crossentropy** loss, which is suitable for multi-class token prediction tasks.\n",
    "\n",
    "So, at the end summary of the architecture is printed, this section to verify the structure and total parameter count as well"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Final Model Architecture test v6.0 --------------------\n",
    "print(\"\\nBuilding Neural Network Model test print\")\n",
    "\n",
    "# Define embedding dimension and hidden size as constants for reuse\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Create encoder-decoder model with named layers for inference model building\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(MAX_LEN,), name='encoder_input')\n",
    "    encoder_embedding_layer = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name='encoder_embedding')\n",
    "    encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "    # Encoder LSTM\n",
    "    encoder_lstm = layers.LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = layers.Input(shape=(MAX_LEN,), name='decoder_input')\n",
    "    decoder_embedding_layer = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name='decoder_embedding')\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "    # Decoder LSTM\n",
    "    decoder_lstm = layers.LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    # Dense layers\n",
    "    dense_layer = layers.Dense(64, activation='relu', name='dense_hidden')\n",
    "    dense_hidden = dense_layer(decoder_outputs)\n",
    "\n",
    "    output_layer = layers.Dense(VOCAB_SIZE, activation='softmax', name='output_layer')\n",
    "    final_outputs = output_layer(dense_hidden)\n",
    "\n",
    "    # Create training model\n",
    "    model = models.Model([encoder_inputs, decoder_inputs], final_outputs, name='infix_to_postfix')\n",
    "\n",
    "    # Store layer references for inference model building\n",
    "    model.encoder_embedding_layer = encoder_embedding_layer\n",
    "    model.encoder_lstm = encoder_lstm\n",
    "    model.decoder_embedding_layer = decoder_embedding_layer\n",
    "    model.decoder_lstm = decoder_lstm\n",
    "    model.dense_layer = dense_layer\n",
    "    model.output_layer = output_layer\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "model = create_model()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Model summary\n",
    "print(\"\\nModel Architecture as a summary\")\n",
    "model.summary()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Model Training Part: Supervised Learning with Teacher Forcing\n",
    "\n",
    "In this section, I train the encoder-decoder model using the generated infix-postfix expression pairs.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- **Input Preparation**:\n",
    "  - **X_train_inputs** contains the encoder input and decoder input sequences (with SOS token prepended).\n",
    "  - **Y_train_targets** contains the expected decoder output (with EOS and padding).\n",
    "  - Similar structure is applied to validation data.\n",
    "\n",
    "- **Callbacks**:\n",
    "  - **EarlyStopping**: Stops training when the validation loss stops improving for 10 epochs, restoring the best weights.\n",
    "  - **ReduceLROnPlateau**: Dynamically reduces learning rate if no progress is observed, improving convergence.\n",
    "\n",
    "- **Training Configurations**:\n",
    "  - Uses **batch_size = 32**, **epochs = 15**, and teacher forcing during training.\n",
    "  - **Sparse categorical crossentropy** is used to match the softmax output with token IDs.\n",
    "\n",
    "- **Model Saving**:\n",
    "  - The trained model is saved in multiple formats:\n",
    "    - Full model (`modelx.keras`)\n",
    "    - Weights only (`modelx.weights.h5`)\n",
    "    - TensorFlow SavedModel format for serving (`modelx_ckpt`)\n",
    "\n",
    "Training metrics are stored in the `history` object for later visualization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Training --------------------\n",
    "print(\"\\nStart Training the Model test print\")\n",
    "\n",
    "# Prepare training data\n",
    "X_train_inputs = [X_train, decoder_input_train]\n",
    "Y_train_targets = np.expand_dims(Y_train, -1)\n",
    "X_val_inputs = [X_val, decoder_input_val]\n",
    "Y_val_targets = np.expand_dims(Y_val, -1)\n",
    "\n",
    "# Training callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training process test print\")\n",
    "history = model.fit(\n",
    "    X_train_inputs, Y_train_targets,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val_inputs, Y_val_targets),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('Infix-to-postfix translation-model-Yasas1.keras')\n",
    "model.save_weights('Infix-to-postfix translation-model-Yasas1.weights.h5')\n",
    "model.export('Infix-to-postfix translation-model-Yasas1_ckpt')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Training History Visualization --------------------\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Pretrained Weights from Google Drive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Download pretrained weights from Google Drive using gdown\n",
    "!pip install -q gdown\n",
    "\n",
    "# Replace this with your actual file ID from Google Drive\n",
    "file_id = \"1QBp1wlIrFyWb8zPEPQIdT4SWL3kkop3v\"\n",
    "output_name = \"Infix-to-postfix-translation-model-Yasas1.weights.h5\"\n",
    "\n",
    "# Download the weights\n",
    "!gdown https://drive.google.com/uc?id={file_id} -O {output_name}"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_weights(\"Infix-to-postfix-translation-model-Yasas1.weights.h5\")\n",
    "print(\"Pretrained weights loaded successfully Test Print\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Inference Models for True Autoregressive Decoding\n",
    "\n",
    "My model is trained using teacher forcing \u2014 where the full sequence is available at each step \u2014 **this approach cannot be used during evaluation** if we aim to simulate real-world prediction behavior. That's why I had to **build separate inference models** for step-by-step, autoregressive decoding.\n",
    "\n",
    "### Why Inference Models is Necessary in my case?\n",
    "\n",
    "During training, we pass the entire decoder input sequence to the model at once. But this **violates true autoregressive decoding**, because:\n",
    "\n",
    "- The decoder LSTM processes the **full sequence** in parallel.\n",
    "- It gains access to future context it wouldn't have during real-time generation.\n",
    "- This leads to unrealistically high evaluation scores and breaks the generation constraints.\n",
    "\n",
    "### So What Proper Autoregressive Decoding Requires?\n",
    "\n",
    "To meet the project constraints and simulate real inference:\n",
    "\n",
    "- I use the encoder to compute the initial state vector.\n",
    "- Then, I use a loop to decode one token at a time.\n",
    "- At each step:\n",
    "  - Feed **only the last predicted token**.\n",
    "  - Update internal decoder LSTM states (`h`, `c`) manually.\n",
    "  - Repeat until an `EOS` token is predicted or a maximum length is reached.\n",
    "\n",
    "This section of code builds:\n",
    "\n",
    "- A **standalone encoder model** to generate the initial hidden and cell states from the infix input.\n",
    "- A **custom decoder model** that:\n",
    "  - Accepts a **single token input** at each step.\n",
    "  - Uses the previously generated decoder states to continue the prediction.\n",
    "  - Outputs both the next token and the updated decoder states.\n",
    "\n",
    "These two models are used together in a custom loop to generate postfix expressions one token at a time \u2014 exactly as project required for **true autoregressive evaluation**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- BUILD INFERENCE MODELS --------------------\n",
    "print(\"\\nBuilding Inference Models for Autoregressive Decoding test print\")\n",
    "\n",
    "def build_inference_models(trained_model):\n",
    "    \"\"\"\n",
    "    Build separate encoder and decoder models for proper autoregressive inference\n",
    "    \"\"\"\n",
    "    # Get the trained model's layers\n",
    "    encoder_inputs = trained_model.input[0]\n",
    "\n",
    "    # Get encoder states from the trained model\n",
    "    encoder_outputs, state_h, state_c = trained_model.get_layer('encoder_lstm').output\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Build encoder model\n",
    "    encoder_model = models.Model(encoder_inputs, encoder_states, name='encoder_inference')\n",
    "\n",
    "    # Build decoder model for inference\n",
    "    # Decoder inputs\n",
    "    decoder_state_input_h = layers.Input(shape=(HIDDEN_SIZE,), name='decoder_h_input')\n",
    "    decoder_state_input_c = layers.Input(shape=(HIDDEN_SIZE,), name='decoder_c_input')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    # Single token input for decoder\n",
    "    decoder_input_single = layers.Input(shape=(1,), name='decoder_single_input')\n",
    "\n",
    "    # Get embeddings for single token\n",
    "    decoder_embedding_inf = trained_model.decoder_embedding_layer(decoder_input_single)\n",
    "\n",
    "    # Run through decoder LSTM\n",
    "    decoder_outputs_inf, state_h_inf, state_c_inf = trained_model.decoder_lstm(\n",
    "        decoder_embedding_inf, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "\n",
    "    # Apply dense layers\n",
    "    dense_outputs_inf = trained_model.dense_layer(decoder_outputs_inf)\n",
    "    decoder_outputs_inf = trained_model.output_layer(dense_outputs_inf)\n",
    "\n",
    "    # Create decoder model\n",
    "    decoder_model = models.Model(\n",
    "        [decoder_input_single] + decoder_states_inputs,\n",
    "        [decoder_outputs_inf] + decoder_states_inf,\n",
    "        name='decoder_inference'\n",
    "    )\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "# Build the inference models\n",
    "encoder_model, decoder_model = build_inference_models(model)\n",
    "\n",
    "print(\"Encoder inference model summary:\")\n",
    "encoder_model.summary()\n",
    "print(\"\\nDecoder inference model summary:\")\n",
    "decoder_model.summary()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoregressive Decoding Function: Token-by-Token Inference\n",
    "\n",
    "This method **strictly enforces autoregressive decoding**, preventing any lookahead or parallel processing during generation, just like a real parser operating in real-time.\n",
    "\n",
    "The result is a decoded postfix expression that aligns with the evaluation constraints of the project and fairly reflects model performance.\n",
    "\n",
    "### Why Not Use Full Sequence Decoding in My Case?\n",
    "\n",
    "Standard decoding during training uses the full sequence of previous tokens, which allows the model to see **all prior context at once** (and even exploit padding). This violates the core constraint of the project, which requires **greedy, step-by-step decoding**, where each token is predicted based only on:\n",
    "\n",
    "- The initial encoder output (hidden states)\n",
    "- The last predicted token\n",
    "- The updated internal states of the decoder\n",
    "\n",
    "### What This part of the code Does\n",
    "\n",
    "- **Initial Step**:\n",
    "  - Runs the encoder model once to obtain initial hidden and cell states from the infix input.\n",
    "  - Initializes the decoder with the `SOS` token.\n",
    "\n",
    "- **Token-by-Token Loop**:\n",
    "  - Predicts one token at a time using the current token and decoder states.\n",
    "  - Updates the decoder input with the sampled token.\n",
    "  - Feeds back the new decoder states (`h`, `c`) for the next prediction.\n",
    "  - Stops if an `EOS` token is reached or maximum length is exceeded."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- AUTOREGRESSIVE DECODE FUNCTION --------------------\n",
    "def autoregressive_decode_proper(encoder_model, decoder_model, encoder_input, max_length=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Proper autoregressive decoding using separate encoder/decoder models\n",
    "    This ensures true step-by-step generation without any possibility of lookahead\n",
    "    \"\"\"\n",
    "    # Prepare encoder input\n",
    "    if encoder_input.ndim == 1:\n",
    "        encoder_input = np.expand_dims(encoder_input, 0)\n",
    "\n",
    "    # Encode the input sequence to get states\n",
    "    states_value = encoder_model.predict(encoder_input, verbose=0)\n",
    "\n",
    "    # Initialize with SOS token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = SOS_ID\n",
    "\n",
    "    # Collect the decoded sequence\n",
    "    decoded_sequence = [SOS_ID]\n",
    "\n",
    "    # Decode step by step\n",
    "    for i in range(max_length - 1):\n",
    "        # Predict next token and update states\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample the next token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_sequence.append(sampled_token_index)\n",
    "\n",
    "        # Exit if EOS\n",
    "        if sampled_token_index == EOS_ID:\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (single token)\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states for next iteration\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return np.array(decoded_sequence)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updated the Test Function and The Reason\n",
    "\n",
    "In the original project notes, the `test()` function used `model.predict()` on both the encoder and decoder inputs together, which **violated the constraint of true autoregressive decoding**.\n",
    "\n",
    "That approach allowed the decoder to:\n",
    "\n",
    "- Process the full decoder input sequence in parallel.\n",
    "- Access future token positions due to teacher forcing mechanics.\n",
    "- Benefit from hidden alignment learned during training.\n",
    "\n",
    "### The Fix: Step-by-Step Generation\n",
    "\n",
    "Updated the test function to use `autoregressive_decode_proper()` with separate encoder and decoder inference models. This ensures:\n",
    "\n",
    "- Only one token is fed at a time to the decoder.\n",
    "- Internal decoder states (`h`, `c`) are updated manually.\n",
    "- Predictions are made **strictly one token at a time**, without lookahead.\n",
    "\n",
    "This change is helps to comply with the project's requirement for **true autoregressive generation** during evaluation and guarantees that the final score fairly reflects the model's real-world performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- EVALUATION FUNCTIONS --------------------\n",
    "def prefix_accuracy_single(y_true, y_pred, id_to_token, eos_id=EOS_ID, verbose=False):\n",
    "    \"\"\"Calculate prefix accuracy between predicted and true sequences\"\"\"\n",
    "    t_str = decode_sequence(y_true, id_to_token).split(' EOS')[0]\n",
    "    p_str = decode_sequence(y_pred, id_to_token).split(' EOS')[0]\n",
    "\n",
    "    t_tokens = t_str.strip().split()\n",
    "    p_tokens = p_str.strip().split()\n",
    "\n",
    "    max_len = max(len(t_tokens), len(p_tokens))\n",
    "    match_len = sum(x == y for x, y in zip(t_tokens, p_tokens))\n",
    "    score = match_len / max_len if max_len > 0 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(\"TARGET :\", ' '.join(t_tokens))\n",
    "        print(\"PREDICT:\", ' '.join(p_tokens))\n",
    "        print(f\"PREFIX MATCH: {match_len}/{len(t_tokens)} \\u2192 {score:.2f}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "def test_with_proper_autoregression(encoder_model, decoder_model, no=20, rounds=10):\n",
    "    \"\"\"Test using proper autoregressive decoding\"\"\"\n",
    "    rscores = []\n",
    "    for i in range(rounds):\n",
    "        print(f\"Round {i+1}/{rounds}\")\n",
    "        X_test, Y_test = generate_dataset(no)\n",
    "        scores = []\n",
    "\n",
    "        for j in range(no):\n",
    "            encoder_input = X_test[j]\n",
    "            # Use the proper autoregressive decoding\n",
    "            generated = autoregressive_decode_proper(encoder_model, decoder_model, encoder_input)[1:]  # remove SOS\n",
    "            scores.append(prefix_accuracy_single(Y_test[j], generated, id_to_token))\n",
    "\n",
    "        rscores.append(np.mean(scores))\n",
    "\n",
    "    return np.mean(rscores), np.std(rscores)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test with proper autoregression\n",
    "print(\"\\nTesting with Proper Autoregressive Decoding test print\")\n",
    "res, std = test_with_proper_autoregression(encoder_model, decoder_model, 20, 10)\n",
    "print(f\"Score: {res:.3f}, Std: {std:.3f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}